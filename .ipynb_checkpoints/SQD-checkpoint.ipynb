{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################bitcoin alpha###################\n",
      "#######################bitcoin otc###################\n",
      "#######################facebook###################\n",
      "#######################kdd###################\n",
      "#######################twitter###################\n",
      "#######################telecoms###################\n",
      "weak edges: 3986\n",
      "k is : 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloud-user/anaconda3/envs/python3/lib/python3.7/site-packages/ipykernel_launcher.py:583: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The iterative calls gave a solution of size: 149\n",
      "removing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloud-user/anaconda3/envs/python3/lib/python3.7/site-packages/ipykernel_launcher.py:315: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/home/cloud-user/anaconda3/envs/python3/lib/python3.7/site-packages/ipykernel_launcher.py:285: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/home/cloud-user/anaconda3/envs/python3/lib/python3.7/site-packages/ipykernel_launcher.py:603: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a qeo with sigma: 3\n",
      "20\n",
      "There is a node with a large predecessor set\n",
      "new_edges:\n",
      "3420\n",
      "the algorithm takes: 12.217997\n",
      "done\n",
      "weak edges: 3986\n",
      "k is : 80\n",
      "The iterative calls gave a solution of size: 149\n",
      "removing...\n",
      "Found a qeo with sigma: 3\n",
      "40\n",
      "There is a node with a large predecessor set\n",
      "new_edges:\n",
      "7524\n",
      "the algorithm takes: 12.115428\n",
      "done\n",
      "weak edges: 3986\n",
      "k is : 399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloud-user/anaconda3/envs/python3/lib/python3.7/site-packages/ipykernel_launcher.py:678: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import Goldberg\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.stats as stat\n",
    "import scipy.sparse as sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import collections\n",
    "import numbers\n",
    "import typing\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_to_wedge_matrix(G, labeling, ind, no_weak_edges):\n",
    "    #To save memory, only build wedge graph on weak edges, strong edges are stored as node weights \n",
    "    \n",
    "#    I=[]\n",
    "#    J=[]\n",
    "#    V=[] \n",
    "    wedge_tuples=[]\n",
    "    cou=0\n",
    "    m=no_weak_edges\n",
    "    node_weights=m*[0]\n",
    "    if m!=len(ind):\n",
    "        print (\"error!\")\n",
    "    \n",
    "    nodes_iter=G.nodes()\n",
    "    for n in nodes_iter:\n",
    "        for (x,y) in itertools.combinations(G.neighbors(n),2):\n",
    "            if not G.has_edge(x,y):\n",
    "#                    if c[n]==c[x] and c[x]==c[y]:\n",
    "                e1=tup(x,n)\n",
    "                e2=tup(n,y)\n",
    "\n",
    "                #both edges are weak\n",
    "                if labeling[e1]==0 and labeling[e2]==0:\n",
    "                    cou+=1\n",
    "#                    I.append(ind[e1])\n",
    "#                    J.append(ind[e2])\n",
    "#                    V.append(1)\n",
    "                    wedge_tuples.append((ind[e1],ind[e2]))\n",
    "                #e1 is weak\n",
    "                if labeling[e1]==0 and labeling[e2]==1:\n",
    "                    node_weights[ind[e1]]+=1\n",
    "                #e2 is weak\n",
    "                if labeling[e1]==1 and labeling[e2]==0:\n",
    "                    node_weights[ind[e2]]+=1\n",
    "\n",
    "#    P = sparse.coo_matrix((V,(I,J)),shape=(m,m))\n",
    "\n",
    "    return np.array(wedge_tuples),cou,node_weights\n",
    "\n",
    "    \n",
    "def tup(v1,v2):\n",
    "    if int(v1)<int(v2):\n",
    "        return (v1,v2)\n",
    "    else:\n",
    "        return (v2,v1)\n",
    "    \n",
    "def is_in_nparray(tupl, nparray):\n",
    "    for elem in nparray:\n",
    "        if elem[0]==tupl[0] and elem[1]==tupl[1]:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def is_independent_to_all(node, adjacencies,nodes):\n",
    "\n",
    "    for othernode in nodes:\n",
    "        if node in adjacencies[othernode]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_adjacent_to_all(node, adjacencies, nodes):\n",
    "    for othernode in nodes:\n",
    "        if node not in adjacencies[othernode]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_min_red_edges_node(redcount, max_possible):\n",
    "    minimum=max_possible\n",
    "    min_node=-1\n",
    "    for node in redcount.keys():\n",
    "        if redcount[node]<=minimum:\n",
    "            minimum=redcount[node]\n",
    "            min_node=node\n",
    "            \n",
    "    return minimum,min_node\n",
    "        \n",
    "def keep_subgraph(edgelist, which_ones):\n",
    "    new_edgelist=[]\n",
    "    for e in edgelist:\n",
    "        if e[0] in which_ones and e[1] in which_ones:\n",
    "            new_edgelist.append(e)\n",
    "    return np.array(new_edgelist)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def join_step(itemsets, adjacencies):\n",
    "    \"\"\"\n",
    "    Join k length itemsets into k + 1 length itemsets.\n",
    "\n",
    "    This algorithm assumes that the list of itemsets are sorted, and that the\n",
    "    itemsets themselves are sorted tuples. Instead of always enumerating all\n",
    "    n^2 combinations, the algorithm only has n^2 runtime for each block of\n",
    "    itemsets with the first k - 1 items equal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    itemsets : list of itemsets\n",
    "        A list of itemsets of length k, to be joined to k + 1 length\n",
    "        itemsets.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> # This is an example from the 1994 paper by Agrawal et al.\n",
    "    >>> itemsets = [(1, 2, 3), (1, 2, 4), (1, 3, 4), (1, 3, 5), (2, 3, 4)]\n",
    "    >>> list(join_step(itemsets))\n",
    "    [(1, 2, 3, 4), (1, 3, 4, 5)]\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    # Iterate over every itemset in the itemsets\n",
    "    while i < len(itemsets):\n",
    "\n",
    "        # The number of rows to skip in the while-loop, intially set to 1\n",
    "        skip = 1\n",
    "\n",
    "        # Get all but the last item in the itemset, and the last item\n",
    "#        itemset_first= itemsets[i][:-1]\n",
    "#        itemset_last=itemsets[i][-1]\n",
    "        *itemset_first, itemset_last = itemsets[i]\n",
    "\n",
    "        # We now iterate over every itemset following this one, stopping\n",
    "        # if the first k - 1 items are not equal. If we're at (1, 2, 3),\n",
    "        # we'll consider (1, 2, 4) and (1, 2, 7), but not (1, 3, 1)\n",
    "\n",
    "        # Keep a list of all last elements, i.e. tail elements, to perform\n",
    "        # 2-combinations on later on\n",
    "        tail_items = [itemset_last]\n",
    "        tail_items_append = tail_items.append  # Micro-optimization\n",
    "\n",
    "        # Iterate over ever itemset following this itemset\n",
    "        for j in range(i + 1, len(itemsets)):\n",
    "\n",
    "            # Get all but the last item in the itemset, and the last item\n",
    "            *itemset_n_first, itemset_n_last = itemsets[j]\n",
    "#            itemset_n_first= itemsets[j][:-1]\n",
    "#            itemset_n_last=itemsets[j][-1]\n",
    "\n",
    "            # If it's the same, append and skip this itemset in while-loop\n",
    "            if itemset_first == itemset_n_first:\n",
    "\n",
    "                # Micro-optimization\n",
    "                tail_items_append(itemset_n_last)\n",
    "                skip += 1\n",
    "\n",
    "            # If it's not the same, break out of the for-loop\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # For every 2-combination in the tail items, yield a new candidate\n",
    "        # itemset, which is sorted.\n",
    "        itemset_first_tuple = tuple(itemset_first)\n",
    "        for a, b in sorted(itertools.combinations(tail_items, 2)):\n",
    "            if is_independent_to_all(a,adjacencies,itemset_first_tuple) and is_independent_to_all(b,adjacencies,itemset_first_tuple) and a not in adjacencies[b]:\n",
    "                yield itemset_first_tuple + (a,) + (b,)\n",
    "\n",
    "        # Increment the while-loop counter\n",
    "        i += skip\n",
    "\n",
    "\n",
    "def prune_step(\n",
    "    itemsets, possible_itemsets):\n",
    "    \"\"\"\n",
    "    Prune possible itemsets whose subsets are not in the list of itemsets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    itemsets : list of itemsets\n",
    "        A list of itemsets of length k.\n",
    "    possible_itemsets : list of itemsets\n",
    "        A list of possible itemsets of length k + 1 to be pruned.\n",
    "\n",
    "    Examples\n",
    "    -------\n",
    "    >>> itemsets = [('a', 'b', 'c'), ('a', 'b', 'd'),\n",
    "    ...             ('b', 'c', 'd'), ('a', 'c', 'd')]\n",
    "    >>> possible_itemsets = list(join_step(itemsets))\n",
    "    >>> list(prune_step(itemsets, possible_itemsets))\n",
    "    [('a', 'b', 'c', 'd')]\n",
    "    \"\"\"\n",
    "\n",
    "    # For faster lookups\n",
    "    itemsets = set(itemsets)\n",
    "\n",
    "    # Go through every possible itemset\n",
    "    for possible_itemset in possible_itemsets:\n",
    "\n",
    "        # Remove 1 from the combination, same as k-1 combinations\n",
    "        # The itemsets created by removing the last two items in the possible\n",
    "        # itemsets must be part of the itemsets by definition,\n",
    "        # due to the way the `join_step` function merges the sorted itemsets\n",
    "\n",
    "        for i in range(len(possible_itemset) - 2):\n",
    "            removed = possible_itemset[:i] + possible_itemset[i + 1 :]\n",
    "\n",
    "            # If every k combination exists in the set of itemsets,\n",
    "            # yield the possible itemset. If it does not exist, then it's\n",
    "            # support cannot be large enough, since supp(A) >= supp(AB) for\n",
    "            # all B, and if supp(S) is large enough, then supp(s) must be large\n",
    "            # enough for every s which is a subset of S.\n",
    "            # This is the downward-closure property of the support function.\n",
    "            if removed not in itemsets:\n",
    "                break\n",
    "\n",
    "        # If we have not breaked yet\n",
    "        else:\n",
    "            yield possible_itemset\n",
    "\n",
    "\n",
    "def apriori_gen(itemsets, adjacencies):\n",
    "\n",
    "    possible_extensions = join_step(itemsets, adjacencies)\n",
    "    yield from prune_step(itemsets, possible_extensions)\n",
    "\n",
    "def construct_subsets(generated_sets, neighs, adjacencies, sigma):\n",
    "#    k=2\n",
    "#    for pair in itertools.combinations(neighs,2):\n",
    "#        #the two nodes are not adjacent, so this tuple can be a candidate\n",
    "#        if pair[0] not in adjacencies[pair[1]]:\n",
    "#            tuples.append(pair)\n",
    "    \n",
    "    \n",
    "        # STEP 2 - Build up the size of the itemsets\n",
    "    # ------------------------------------------\n",
    "    prev_itemsets=[(i,) for i in neighs]\n",
    "    # While there are itemsets of the previous size\n",
    "    k = 2\n",
    "    while (k<=sigma+1):\n",
    "#        print(\" Counting itemsets of length {}.\".format(k))\n",
    "\n",
    "        # STEP 2a) - Build up candidate of larger itemsets\n",
    "\n",
    "        # Generate candidates of length k + 1 by joning, prune, and copy as set\n",
    "        C_k = list(apriori_gen(prev_itemsets, adjacencies))\n",
    "\n",
    "        # If no candidate itemsets were found, break out of the loop\n",
    "        if not C_k:\n",
    "            return\n",
    "            \n",
    "\n",
    "        prev_itemsets=C_k       \n",
    "\n",
    "        k += 1\n",
    "\n",
    "        \n",
    "    for itemset in prev_itemsets:\n",
    "        if itemset not in generated_sets:\n",
    "            generated_sets.add(itemset)\n",
    "\n",
    "\n",
    "    return\n",
    "\n",
    "    \n",
    "            \n",
    "\n",
    "def construct_bipartite_graph(adjacencies, selected_nodes, sigma):\n",
    "    Bipartite=nx.Graph()    \n",
    "    cou=max(selected_nodes)+1\n",
    "    generated_sets=set()\n",
    "            \n",
    "    #create subsets based on neighbourhood of every node\n",
    "    for node in selected_nodes:\n",
    "        neighs=adjacencies[node]\n",
    "        Bipartite.add_node(node)\n",
    "        construct_subsets(generated_sets, neighs, adjacencies, sigma)  \n",
    "        \n",
    "    how_many_subsets=0\n",
    "    A={}\n",
    "    start_time = time.clock()\n",
    "    \n",
    "    for subset in generated_sets:\n",
    "        how_many_subsets+=1\n",
    "        A[cou]=subset\n",
    "        for node in selected_nodes:\n",
    "            if is_adjacent_to_all(node, adjacencies, subset):\n",
    "                Bipartite.add_edge(node,cou, color='red')\n",
    "            if node in subset:\n",
    "                Bipartite.add_edge(node,cou, color='black')\n",
    "            \n",
    "        cou+=1\n",
    "#    print(\"About to print bipartite edges!\")\n",
    "#    print(Bipartite.edges())\n",
    "#    for e in Bipartite.edges():\n",
    "#        print(e)\n",
    "#        print ( str( (e[0],A[e[1]])) +Bipartite[e[0]][e[1]]['color'])\n",
    "    \n",
    "    return Bipartite,how_many_subsets\n",
    "    \n",
    "    \n",
    "\n",
    "def compute_qeo(subgraph, selected_nodes, sigma):\n",
    "#    selected_nodes=selected_nodes[10:15]\n",
    "#    subgraph=keep_subgraph(subgraph,selected_nodes)\n",
    "#    print subgraph\n",
    "#    subgraph=set(subgraph_list)\n",
    "        \n",
    "\n",
    "    #Build bipartite graph as described by Ye and Borodin\n",
    "    start_time = time.clock()\n",
    "    \n",
    "    #transform edgelist to adjacency list\n",
    "    adjacencies={}\n",
    "    for e in subgraph:\n",
    "        v1=e[0]\n",
    "        v2=e[1]\n",
    "        if v1 not in adjacencies:\n",
    "            adjacencies[v1]=[]\n",
    "        if v2 not in adjacencies:\n",
    "            adjacencies[v2]=[]\n",
    "        adjacencies[v1].append(v2)\n",
    "        adjacencies[v2].append(v1)\n",
    "\n",
    "    Bipartite,how_many_subsets=construct_bipartite_graph(adjacencies, selected_nodes, sigma)    \n",
    "\n",
    "#    print( 'ellapsed seconds to build bipartite graph {}'.format(time.clock() - start_time))\n",
    "    qeo={}\n",
    "    redcount={}\n",
    "    #count how many red edges are adjacent to each node\n",
    "    for nodeB in selected_nodes:\n",
    "        if nodeB not in redcount:\n",
    "            redcount[nodeB]=0\n",
    "        for nodeA in Bipartite.neighbors(nodeB):\n",
    "            if Bipartite[nodeB][nodeA]['color']=='red':\n",
    "                redcount[nodeB]+=1\n",
    "                \n",
    "#    print (redcount)\n",
    "    #start removing nodes\n",
    "    for i in range(len(selected_nodes)):\n",
    "        num_red_edges,min_red_edges_node=find_min_red_edges_node(redcount, how_many_subsets)\n",
    "        if num_red_edges>0:\n",
    "            print (\"There does not exist a qeo with sigma: {}\".format(sigma))\n",
    "            return qeo,[]\n",
    "        #remove all neighbors\n",
    "        neighs=list(Bipartite.neighbors(min_red_edges_node))\n",
    "        for node in neighs:\n",
    "            #update red edges counter for all deleted red edges adjacent to node from set A that will be deleted\n",
    "            for nodeB in Bipartite.neighbors(node):\n",
    "                if Bipartite[nodeB][node]['color']=='red':\n",
    "                    redcount[nodeB]-=1\n",
    "            #remove node from set A\n",
    "            Bipartite.remove_node(node)\n",
    "        #remove the node\n",
    "        Bipartite.remove_node(min_red_edges_node)\n",
    "        del redcount[min_red_edges_node]\n",
    "        qeo[min_red_edges_node]=i\n",
    "        \n",
    "        \n",
    "    #process adjacency list to convert it to predecessor list in the qeo\n",
    "    \n",
    "    for i in selected_nodes:\n",
    "        order=qeo[i]\n",
    "        adjacencies[i]=[j for j in adjacencies[i] if qeo[j]<qeo[i]]\n",
    "                \n",
    "    \n",
    "    print (\"Found a qeo with sigma: {}\".format(sigma))\n",
    "    return qeo, adjacencies\n",
    "\n",
    "\n",
    "def count_edges(W, d_vF, nodes):\n",
    "    new_edges=0\n",
    "    for e in W:\n",
    "        if e[0] in nodes and e[1] in nodes:\n",
    "            new_edges+=1\n",
    "#    print(\"new edges from weak edges: \"+str(new_edges))\n",
    "    for node in nodes:\n",
    "        new_edges+=d_vF[node]\n",
    "        \n",
    "    return new_edges\n",
    "    \n",
    "    \n",
    "    \n",
    "def remove_nodes(k, subgraph,selected_nodes, sigma, d_vF):\n",
    "    qeo,predecessor_list=compute_qeo(subgraph,selected_nodes,sigma)\n",
    "    thresh=math.floor(k/2.0)\n",
    "    dense_subset=[]\n",
    "    #check if there is a node with a large predecessor set\n",
    "    print(thresh)\n",
    "    for node in predecessor_list.keys():\n",
    "        if len(predecessor_list[node])>=thresh:\n",
    "            dense_subset=predecessor_list[node]\n",
    "            print(\"There is a node with a large predecessor set\")\n",
    "            break\n",
    "            \n",
    "    if len(dense_subset)>0:\n",
    "        feasible_subset=random.sample(dense_subset,k=thresh)\n",
    "#        print(feasible_subset)\n",
    "        sorted_indices=np.argsort(d_vF)[::-1]\n",
    "        for node in sorted_indices:\n",
    "            if node not in feasible_subset and len(feasible_subset)<k:\n",
    "                feasible_subset.append(node)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#    print(qeo)\n",
    "    return feasible_subset\n",
    "\n",
    "\n",
    "def is_tuple_eq(tup1, tup2):\n",
    "    if tup1[0]==tup2[0] and tup1[1]==tup2[1]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def take_subgraph(edgelist, nodes):\n",
    "    subgraph=[]\n",
    "    for e in edgelist:\n",
    "        if e[0] in nodes and e[1] in nodes:\n",
    "            subgraph.append(e)\n",
    "    return np.array(subgraph)\n",
    "\n",
    "    \n",
    "def iterative_calls(W,no_wedges,node_weights, k):\n",
    "    removed_edges=[]\n",
    "    selected_nodes=[]\n",
    "    subgraph=[]\n",
    "    num_removed_nodes=0\n",
    "    iters=0\n",
    "#    print (\"in the first iteration node_weights has this many elements: {}\".format(len(node_weights) ))\n",
    "    while num_removed_nodes<k/2.0:\n",
    "        iters+=1\n",
    "#        print(\"Iterative call: \"+str(iters))\n",
    "#        print(\"selected nodes: \"+str(selected_nodes))\n",
    "#        print(\"node weights: \"+str(set(zip(range(len(node_weights)),node_weights))))\n",
    "#        print(W)\n",
    "        new_selected_nodes=Goldberg.Densest_Subgraph(W,no_wedges,node_weights, selected_nodes)\n",
    "#        print (\"selected to remove: {}\".format(new_selected_nodes))\n",
    "            \n",
    "        #remove selected nodes and update weight of their neighbours\n",
    "        cou=0\n",
    "        indices_to_remove=[]\n",
    "        for tupl in W:\n",
    "            i=tupl[0]\n",
    "            j=tupl[1]\n",
    "            if i in new_selected_nodes:\n",
    "                node_weights[i]=0\n",
    "                if j in new_selected_nodes:\n",
    "                    node_weights[j]=0\n",
    "                else:\n",
    "                    node_weights[j]+=1\n",
    "                indices_to_remove.append(cou)\n",
    "                removed_edges.append(W[cou])\n",
    "            elif j in new_selected_nodes:\n",
    "                #j is in removed nodes by i is not\n",
    "                node_weights[j]=0\n",
    "                node_weights[i]+=1\n",
    "                indices_to_remove.append(cou)\n",
    "                removed_edges.append(W[cou])\n",
    "            cou+=1\n",
    " #       print(\"len of W before removal: \"+str(len(W)))\n",
    "        W=np.delete(W,indices_to_remove,axis=0)\n",
    "#        print(\"len of W after removal: \"+str(len(W)))\n",
    "        no_wedges=no_wedges-len(indices_to_remove)\n",
    "        \n",
    "        selected_nodes=selected_nodes+new_selected_nodes\n",
    "#        print(\"selected so far: {}\".format(selected_nodes))\n",
    "        num_removed_nodes=len(selected_nodes)\n",
    "#        print(\"I have removed so far {} nodes\".format(num_removed_nodes))\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    return selected_nodes\n",
    "    \n",
    "    \n",
    "    \n",
    "#dataset='edgelist'\n",
    "dataset_directory='/home/cloud-user/Datasets/stc/LesMis/'\n",
    "delimiter=','\n",
    "\n",
    "#dataset='edgelist'\n",
    "#dataset_directory='/home/cloud-user/Datasets/stc/Bigdatasets/bitcoinalpha/'\n",
    "#delimiter=','\n",
    "print(\"#######################bitcoin alpha###################\")\n",
    "#dataset_directory='/home/cloud-user/Datasets/stc/Bigdatasets/bitcoinotc/'\n",
    "#delimiter=','\n",
    "print(\"#######################bitcoin otc###################\")\n",
    "#dataset_directory='/home/cloud-user/Datasets/stc/Bigdatasets/authors/'\n",
    "#delimiter='\\t'\n",
    "#dataset_directory='/home/cloud-user/Datasets/stc/Bigdatasets/facebooksmall2/'\n",
    "#delimiter=';'\n",
    "#dataset_directory='/home/cloud-user/Datasets/stc/Bigdatasets/facebook/'\n",
    "#delimiter=';'\n",
    "print(\"#######################facebook###################\")\n",
    "#main_code()\n",
    "#dataset_directory='/home/cloud-user/Datasets/stc/Bigdatasets/KDD/'\n",
    "#delimiter=' '\n",
    "print(\"#######################kdd###################\")\n",
    "#dataset_directory='/home/cloud-user/Datasets/stc/LesMis/'\n",
    "#delimiter=','\n",
    "dataset_directory='/home/cloud-user/Datasets/stc/Bigdatasets/twitter/'\n",
    "delimiter=';'\n",
    "print(\"#######################twitter###################\")\n",
    "#dataset_directory='/home/cloud-user/Datasets/stc/Bigdatasets/Retweetssmall2/'\n",
    "#delimiter=';'\n",
    "#dataset_directory='/home/cloud-user/Datasets/stc/Bigdatasets/Telecoms/'\n",
    "#delimiter=';'\n",
    "print(\"#######################telecoms###################\")\n",
    "\n",
    "\n",
    "for ka in range(1,3,1):\n",
    "\n",
    "\n",
    "    dataset='edgelist'\n",
    "\n",
    "    G = nx.Graph()\n",
    "    labeling={}\n",
    "    #n=len(G.nodes())\n",
    "    edgelist=open(dataset_directory+dataset, 'r')\n",
    "    weights=[]\n",
    "\n",
    "    for line in edgelist:\n",
    "        splitted=line.strip().split(delimiter)\n",
    "        v1=int(splitted[0])\n",
    "        v2=int(splitted[1])\n",
    "        tupl=tup(v1,v2)\n",
    "        weight=float(splitted[2])\n",
    "        G.add_edge(tupl[0],tupl[1], weight=weight)\n",
    "        weights.append(weight)\n",
    "\n",
    "\n",
    "    G = max(nx.connected_component_subgraphs(G), key=len)\n",
    "    #print(nx.transitivity(G))  \n",
    "    #return\n",
    "    G=nx.convert_node_labels_to_integers(G, label_attribute=\"old\")\n",
    "\n",
    "    ########Split into Strong and Weak###############\n",
    "\n",
    "    splitting=np.percentile(weights, 70)\n",
    "    del(weights)\n",
    "#    print(splitting)\n",
    "    nr_strong_edges=0\n",
    "    nr_weak_edges=0  \n",
    "    edge_to_ind={}  \n",
    "\n",
    "    for e in G.edges():\n",
    "        if G[e[0]][e[1]]['weight']>splitting:\n",
    "            #assign to strong\n",
    "            labeling[e]=1\n",
    "            nr_strong_edges+=1\n",
    "        else:\n",
    "            #assign to weak\n",
    "            labeling[e]=0\n",
    "            edge_to_ind[e]=nr_weak_edges\n",
    "            nr_weak_edges+=1\n",
    "#    print('strong edges: '+str(nr_strong_edges))\n",
    "    print('weak edges: '+str(nr_weak_edges))\n",
    "\n",
    "\n",
    "\n",
    "    n=len(G.nodes())\n",
    "    m=len(G.edges())\n",
    "\n",
    "\n",
    "#    print('nodes: ' +str(n))\n",
    "#    print('edges: ' +str(m))\n",
    "\n",
    "    k=int(round(0.01*ka*nr_weak_edges))\n",
    "    print(\"k is : \"+str(k))\n",
    "\n",
    "    W,no_wedges,node_weights=convert_to_wedge_matrix(G,labeling,edge_to_ind,nr_weak_edges)\n",
    "    d_vF=node_weights.copy()\n",
    "    #node_weights=nr_weak_edges*[1]\n",
    "#    print(no_wedges)\n",
    "    G.clear()\n",
    "    start_time = time.clock()\n",
    "\n",
    "    selected_nodes=iterative_calls(W.copy(),no_wedges,node_weights, k)\n",
    "    subgraph=take_subgraph(W,selected_nodes)\n",
    "    print(\"The iterative calls gave a solution of size: \"+str(len(selected_nodes)))\n",
    "    sigma=2\n",
    "    if len(selected_nodes) > k: \n",
    "        #we need to remove some nodes\n",
    "        print (\"removing...\")\n",
    "        solution=remove_nodes(k, subgraph, selected_nodes, sigma, d_vF)\n",
    "    else: \n",
    "        print(\"no need to remove nodes\")\n",
    "        sorted_indices=np.argsort(d_vF)[::-1]\n",
    "        solution=list(selected_nodes)\n",
    "        for node in sorted_indices:\n",
    "            if node not in solution and len(solution)<k:\n",
    "                solution.append(node)\n",
    "\n",
    "    print(\"new_edges:\")\n",
    "    print(count_edges(W, d_vF, solution))\n",
    "    print (\"the algorithm takes: \"+str(time.clock() - start_time))\n",
    "\n",
    "    print(\"done\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset='edgelist'\n",
    "\n",
    "G = nx.Graph()\n",
    "labeling={}\n",
    "#n=len(G.nodes())\n",
    "edgelist=open(dataset_directory+dataset, 'r')\n",
    "weights=[]\n",
    "\n",
    "for line in edgelist:\n",
    "    splitted=line.strip().split(delimiter)\n",
    "    v1=int(splitted[0])\n",
    "    v2=int(splitted[1])\n",
    "    tupl=tup(v1,v2)\n",
    "    weight=float(splitted[2])\n",
    "    G.add_edge(tupl[0],tupl[1], weight=weight)\n",
    "    weights.append(weight)\n",
    "\n",
    "\n",
    "G = max(nx.connected_component_subgraphs(G), key=len)\n",
    "#print(nx.transitivity(G))  \n",
    "#return\n",
    "G=nx.convert_node_labels_to_integers(G, label_attribute=\"old\")\n",
    "\n",
    "########Split into Strong and Weak###############\n",
    "\n",
    "splitting=np.percentile(weights, 70)\n",
    "del(weights)\n",
    "#    print(splitting)\n",
    "nr_strong_edges=0\n",
    "nr_weak_edges=0  \n",
    "edge_to_ind={}  \n",
    "\n",
    "for e in G.edges():\n",
    "    if G[e[0]][e[1]]['weight']>splitting:\n",
    "        #assign to strong\n",
    "        labeling[e]=1\n",
    "        nr_strong_edges+=1\n",
    "    else:\n",
    "        #assign to weak\n",
    "        labeling[e]=0\n",
    "        edge_to_ind[e]=nr_weak_edges\n",
    "        nr_weak_edges+=1\n",
    "#    print('strong edges: '+str(nr_strong_edges))\n",
    "print('weak edges: '+str(nr_weak_edges))\n",
    "\n",
    "\n",
    "\n",
    "n=len(G.nodes())\n",
    "m=len(G.edges())\n",
    "\n",
    "\n",
    "#    print('nodes: ' +str(n))\n",
    "#    print('edges: ' +str(m))\n",
    "\n",
    "k=int(round(0.1*nr_weak_edges))\n",
    "print(\"k is : \"+str(k))\n",
    "\n",
    "W,no_wedges,node_weights=convert_to_wedge_matrix(G,labeling,edge_to_ind,nr_weak_edges)\n",
    "d_vF=node_weights.copy()\n",
    "#node_weights=nr_weak_edges*[1]\n",
    "#    print(no_wedges)\n",
    "G.clear()\n",
    "start_time = time.clock()\n",
    "\n",
    "selected_nodes=iterative_calls(W.copy(),no_wedges,node_weights, k)\n",
    "subgraph=take_subgraph(W,selected_nodes)\n",
    "print(\"The iterative calls gave a solution of size: \"+str(len(selected_nodes)))\n",
    "sigma=2\n",
    "if len(selected_nodes) > k: \n",
    "    #we need to remove some nodes\n",
    "    print (\"removing...\")\n",
    "    solution=remove_nodes(k, subgraph, selected_nodes, sigma, d_vF)\n",
    "else: \n",
    "    print(\"no need to remove nodes\")\n",
    "    sorted_indices=np.argsort(d_vF)[::-1]\n",
    "    solution=list(selected_nodes)\n",
    "    for node in sorted_indices:\n",
    "        if node not in solution and len(solution)<k:\n",
    "            solution.append(node)\n",
    "\n",
    "print(\"new_edges:\")\n",
    "print(count_edges(W, d_vF, solution))\n",
    "print (\"the algorithm takes: \"+str(time.clock() - start_time))\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
